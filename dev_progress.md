# MQI Communicator Debugging and Refinement Log

This document tracks the logical code flow analysis, identified issues, and resolutions for the MQI Communicator application.

## Initial State (Before Changes)

*   **Codebase:** A Python application designed to watch a directory for new cases, submit them to an HPC via `pueue`, and track their status in a SQLite database.
*   **Components:**
    *   `main.py`: Main application entry point and control loop.
    *   `case_scanner.py`: Watches for new directories using `watchdog`.
    *   `workflow_submitter.py`: Submits jobs to HPC using `scp` and `ssh`.
    *   `db_manager.py`: Manages the SQLite database state.
    *   `config.yaml`: Application configuration.

---

## Trace 1: Application Startup

### Code Flow Analysis
1.  Execution begins in `src/main.py` at the `if __name__ == "__main__"` block.
2.  The `config.yaml` file is opened and parsed to create an `initial_config` object.
3.  `setup_logging(initial_config)` is called, which correctly configures the root logger with file and stream handlers.
4.  The `main()` function is called.
5.  Inside `main()`, the `config.yaml` file is opened and parsed **a second time**.
6.  `setup_logging(config)` is called **a second time**.

### Problem 1: Duplicate Logging
*   **Issue:** Calling `setup_logging()` twice causes a second set of identical handlers to be added to the root logger. As a result, every log message generated by the application will be duplicated in both the console output and the log file. This makes logs noisy and difficult to read.
*   **Solution:** Refactor the entry point to load the configuration and set up logging only once. The loaded `config` object will be passed as an argument to the `main` function, which will no longer be responsible for these setup tasks.

### Resolution for Problem 1
*   **Action:** Modified `src/main.py`.
    1.  Changed the `main` function signature to `main(config: Dict[str, Any]) -> None`.
    2.  Removed the redundant configuration loading and the second call to `setup_logging()` from within the `main` function.
    3.  Updated the call in the `if __name__ == "__main__"` block to `main(initial_config)`.
*   **Status:** **Resolved.**

---

## Trace 2: Case Ingestion and Processing

### Code Flow Analysis
1.  A new directory is copied into the `watch_path` (`new_cases`).
2.  `CaseScanner` detects file system events and uses a debouncing timer to wait for the directory to become "stable" (no new events for 5 seconds).
3.  After the delay, the timer calls `_process_directory` with the path to the new case.
4.  `_process_directory` adds the case to the database with `status='submitted'`.
5.  The main loop picks up the `submitted` case.
6.  It atomically locks an available GPU resource using `db_manager.find_and_lock_any_available_gpu`.
7.  It calls `workflow_submitter.submit_workflow` to `scp` the files and `ssh` to run `pueue add`.
8.  The case status is updated to `running`.

### Problem 2: Ghost Case Creation
*   **Issue:** The `_process_directory` method in `case_scanner.py` does not verify that the directory path still exists on the filesystem before adding it to the database. If a directory is created and then quickly deleted before the 5-second stability timer fires, a "ghost" case will be created in the database for a non-existent directory.
*   **Impact:** This ghost case will be picked up by the main loop, wastefully lock a GPU resource, and then fail during the `scp` transfer. The system recovers by marking the case as `failed`, but it's inefficient and creates confusing "failed" entries in the database.
*   **Solution:** Add a check in `_process_directory` in `src/services/case_scanner.py` to ensure the directory `os.path.isdir()` is true before attempting to add it to the database.
*   **Status:** **Resolved.**

---
